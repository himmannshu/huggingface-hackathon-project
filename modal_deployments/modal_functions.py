import modal
import os
import tempfile
from pathlib import Path


# Create Modal app
app = modal.App("youtube-content-optimizer")

# Define the Modal image with FFmpeg for audio extraction
audio_image = modal.Image.debian_slim().apt_install(
    "ffmpeg"
).pip_install(
    "requests"
)

# Define the Modal image with Whisper for transcription
whisper_image = modal.Image.debian_slim().apt_install(
    "ffmpeg"  # Keep ffmpeg for audio handling
).pip_install(
    "openai-whisper",  # Let pip resolve compatible versions
    "torch",
    "torchaudio"
)

@app.function(
    image=audio_image,
    timeout=600,  # 10 minutes timeout for large videos
    memory=2048,  # 2GB memory for processing
)
def process_video_to_audio(video_bytes: bytes, filename: str) -> tuple[bytes, str]:
    """
    Step 1 & 2: Process uploaded video and extract audio
    
    Args:
        video_bytes: Raw video file bytes from Gradio upload
        filename: Original filename for format detection
        
    Returns:
        tuple: (audio_bytes, audio_filename)
    """
    
    print(f"ğŸ¬ Processing video: {filename}")
    print(f"ğŸ“¦ Video size: {len(video_bytes) / (1024*1024):.1f} MB")
    
    # Create temporary directory for processing
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir)
        
        # Step 1: Save uploaded video temporarily
        video_path = temp_path / f"input_video{Path(filename).suffix}"
        with open(video_path, "wb") as f:
            f.write(video_bytes)
        
        print(f"âœ… Video saved temporarily: {video_path}")
        
        # Step 2: Extract audio using FFmpeg
        audio_filename = f"extracted_audio_{Path(filename).stem}.wav"
        audio_path = temp_path / audio_filename
        
        # FFmpeg command optimized for Whisper
        ffmpeg_cmd = [
            "ffmpeg",
            "-i", str(video_path),
            "-vn",  # No video
            "-acodec", "pcm_s16le",  # PCM 16-bit (good for Whisper)
            "-ar", "16000",  # 16kHz sample rate (Whisper optimal)
            "-ac", "1",  # Mono channel
            "-y",  # Overwrite output
            str(audio_path)
        ]
        
        print(f"ğŸ”„ Running FFmpeg extraction...")
        
        # Execute FFmpeg
        import subprocess
        try:
            result = subprocess.run(
                ffmpeg_cmd,
                capture_output=True,
                text=True,
                check=True
            )
            print(f"âœ… Audio extraction successful!")
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ FFmpeg error: {e}")
            print(f"FFmpeg stderr: {e.stderr}")
            raise Exception(f"Audio extraction failed: {e.stderr}")
        
        # Read extracted audio
        if not audio_path.exists():
            raise Exception("Audio file was not created")
            
        with open(audio_path, "rb") as f:
            audio_bytes = f.read()
        
        print(f"ğŸµ Audio extracted: {len(audio_bytes) / (1024*1024):.1f} MB")
        print(f"ğŸ“ Original video deleted from temporary storage")
        
        return audio_bytes, audio_filename

# Simplified Whisper transcription function with compatibility fixes
@app.function(
    image=whisper_image,
    gpu="A10G",  # Nvidia A10G GPU as requested
    memory=12288,  # 12GB RAM (12 * 1024 MiB)
    timeout=900,  # 15 minutes timeout for transcription
)
def transcribe_audio_with_whisper(audio_bytes: bytes, audio_filename: str) -> dict:
    """
    Step 3: Transcribe audio using OpenAI Whisper (small model)
    
    Args:
        audio_bytes: Raw audio file bytes from audio extraction
        audio_filename: Audio filename for reference
        
    Returns:
        dict: Structured transcription with text, segments, language, duration
    """
    
    print(f"ğŸ™ï¸ Starting Whisper transcription: {audio_filename}")
    print(f"ğŸ“¦ Audio size: {len(audio_bytes) / (1024*1024):.1f} MB")
    
    # Create temporary directory for processing
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir)
        
        # Save audio bytes to temporary file
        audio_path = temp_path / audio_filename
        with open(audio_path, "wb") as f:
            f.write(audio_bytes)
        
        print(f"âœ… Audio saved for transcription: {audio_path}")
        
        # Load Whisper model (small for speed)
        import whisper
        print(f"ğŸ”„ Loading Whisper small model...")
        model = whisper.load_model("small")
        print(f"âœ… Whisper model loaded successfully!")
        
        # Transcribe audio with compatible settings
        print(f"ğŸ”„ Transcribing audio...")
        result = model.transcribe(
            str(audio_path),
            verbose=False,  # Disable verbose to avoid potential issues
            # Remove word_timestamps=True as it causes Triton compatibility issues
            temperature=0.0,  # Use deterministic decoding
            no_speech_threshold=0.6,  # Improve silence detection
            logprob_threshold=-1.0,  # Improve quality filtering
            compression_ratio_threshold=2.4  # Improve repetition filtering
        )
        
        # Extract transcription data
        transcription_data = {
            "text": result["text"].strip(),
            "language": result["language"],
            "segments": [
                {
                    "start": segment["start"],
                    "end": segment["end"], 
                    "text": segment["text"].strip()
                }
                for segment in result["segments"]
            ],
            "duration": result["segments"][-1]["end"] if result["segments"] else 0.0,
            "word_count": len(result["text"].split()),
            "audio_filename": audio_filename
        }
        
        print(f"âœ… Transcription complete!")
        print(f"ğŸ“ Language detected: {transcription_data['language']}")
        print(f"â±ï¸ Duration: {transcription_data['duration']:.1f} seconds")
        print(f"ğŸ“„ Word count: {transcription_data['word_count']} words")
        print(f"ğŸ“ Temporary audio file deleted")
        
        return transcription_data

