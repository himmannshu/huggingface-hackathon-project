# --- Ollama Configuration ---
# MODEL_ENDPOINT: The base URL for your Ollama server.
# Example: http://localhost:11434 if Ollama is running locally.
# If using a Modal deployment for Ollama, this will be your Modal app's URL.
MODEL_ENDPOINT=http://localhost:11434

# MODEL_NAME: The specific Ollama model to use for text generation.
# Ensure the model is downloaded in your Ollama instance (e.g., via `ollama pull llama3`).
# Recommended models: llama3, mistral, mistral:instruct, llama3:instruct, or other capable instruction-following models.
# Using older or smaller models might lead to suboptimal text generation quality.
MODEL_NAME=llama3:8b # Or llama3:latest, mistral:latest etc.

# --- Whisper (Transcription) Configuration ---
# WHISPER_MODEL_SIZE: Specifies the size of the Whisper model to use for audio transcription.
# Options: tiny, base, small, medium, large, large-v1, large-v2, large-v3.
# - Smaller models (e.g., 'tiny', 'base', 'small') are faster and use fewer resources but may have lower accuracy.
# - Larger models (e.g., 'medium', 'large-v3') offer higher accuracy but are slower and consume more resources (CPU, GPU memory, RAM).
# - 'small' is the default and offers a good balance for many use cases.
# The chosen model will be downloaded by Whisper on its first use if not already present in the Modal environment.
WHISPER_MODEL_SIZE=small

# --- YouTube API Configuration ---
# YOUTUBE_API_KEY: Your Google Cloud YouTube Data API v3 key.
# Required for the YouTube Research Agent (Step 5) to fetch competitive video data.
# If not provided, Step 5 will be skipped.
# YOUTUBE_API_KEY=YOUR_YOUTUBE_API_KEY_HERE

# --- Modal specific (if deploying Ollama via Modal) ---
# If you have a separate Modal deployment for your Ollama instance,
# you might have a different endpoint for it.
# MODAL_OLLAMA_ENDPOINT=https://your-ollama-app-name.modal.run
# The application currently uses MODEL_ENDPOINT for Ollama access.
# The original MODAL_ENDPOINT was likely for the main app, not Ollama.
# For clarity, if using Modal for Ollama, set MODEL_ENDPOINT to that Modal URL.
